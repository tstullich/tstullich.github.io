<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Portfolio :: Tilt Shift</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="All of the source code for the listed projects can be viewed on my Github profile or you can find a link to each individual repository at the bottom of each project.
Personal Projects 1. Vulkan Deferred Renderer C&#43;&#43; Vulkan GLSL C&#43;&#43; Dear ImGui Rendering GUI Shaders
The first triangle has been rendered!  I am currently working on building out a deferred renderer using Vulkan and Dear ImGui. My intent is to improve my knowledge around Vulkan and realtime rendering, while also providing a way for others to see what it&amp;rsquo;s like building a renderer from scratch." />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="/portfolio/" />







<link rel="stylesheet" href="/css/style.min.css">


  
  
  <link rel="stylesheet" href="/css/color/green.min.css">






<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/img/apple-touch-icon-144-precomposed.png">

  <link rel="shortcut icon" href="/">



<script src="https://kit.fontawesome.com/4a6cf05255.js" crossorigin="anonymous"></script>


<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="tstullich" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Portfolio :: Tilt Shift">
<meta property="og:description" content="All of the source code for the listed projects can be viewed on my Github profile or you can find a link to each individual repository at the bottom of each project.
Personal Projects 1. Vulkan Deferred Renderer C&#43;&#43; Vulkan GLSL C&#43;&#43; Dear ImGui Rendering GUI Shaders
The first triangle has been rendered!  I am currently working on building out a deferred renderer using Vulkan and Dear ImGui. My intent is to improve my knowledge around Vulkan and realtime rendering, while also providing a way for others to see what it&amp;rsquo;s like building a renderer from scratch." />
<meta property="og:url" content="/portfolio/" />
<meta property="og:site_name" content="Portfolio" />

  <meta property="og:image" content="/">

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">













</head>
<body class="green">


<div class="container center">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Tilt Shift
  </div>
</a>

    </div>
    <div class="menu-trigger">menu</div>
  </div>
  
    <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about">About</a></li>
        
      
        
          <li><a href="/portfolio">Portfolio</a></li>
        
      
        
          <li><a href="/posts">Posts</a></li>
        
      
      
    

    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about">About</a></li>
      
    
      
        <li><a href="/portfolio">Portfolio</a></li>
      
    
      
        <li><a href="/posts">Posts</a></li>
      
    
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="/portfolio/">Portfolio</a></h1>
  <div class="post-meta">
    
    
  </div>

  

  

  
    <div class="table-of-contents">
      <h2>
        
          Table of Contents
        
      </h2>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#personal-projects">Personal Projects</a>
      <ul>
        <li><a href="#1-vulkan-deferred-renderer">1. Vulkan Deferred Renderer</a></li>
        <li><a href="#2-odin---a-vulkan-based-path-tracer">2. Odin - A Vulkan-based Path Tracer</a></li>
        <li><a href="#3-raytracing-in-one-weekend-in-rust">3. Raytracing In One Weekend In Rust</a></li>
      </ul>
    </li>
    <li><a href="#academic-projects">Academic Projects</a>
      <ul>
        <li><a href="#1-masters-thesis-on-differentiable-rendering">1. Master&rsquo;s Thesis On Differentiable Rendering</a></li>
        <li><a href="#3-computer-graphics-2-tu-berlin">3. Computer Graphics 2 (TU Berlin)</a></li>
        <li><a href="#4-computer-graphics-1-tu-berlin">4. Computer Graphics 1 (TU Berlin)</a></li>
      </ul>
    </li>
    <li><a href="#conference-papers">Conference Papers</a>
      <ul>
        <li><a href="#edbt-2019-demonstration-paper-publication">EDBT 2019 Demonstration Paper Publication</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
  

  <div class="post-content"><div>
        <p>All of the source code for the listed projects can be viewed on my
<a href="https://github.com/tstullich">Github profile</a> or you can
find a link to each individual repository at the bottom of each project.</p>
<h2 id="personal-projects">Personal Projects<a href="#personal-projects" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<h3 id="1-vulkan-deferred-renderer">1. Vulkan Deferred Renderer<a href="#1-vulkan-deferred-renderer" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p><code>C++</code> <code>Vulkan</code> <code>GLSL</code> <code>C++</code> <code>Dear ImGui</code> <code>Rendering</code> <code>GUI</code> <code>Shaders</code></p>

  <figure class="center" >
    <img src="/img/triangle-ui.png"  alt="Vulkan Renderer"   style="border-radius: 8px;"  />
    
      <figcaption class="center" >The first triangle has been rendered!</figcaption>
    
  </figure>


<p>I am currently working on building out a deferred renderer using Vulkan and Dear ImGui. My intent
is to improve my knowledge around Vulkan and realtime rendering, while also providing
a way for others to see what it&rsquo;s like building a renderer from scratch. I plan to publish
the source code so that others can freely use it and extend code if they wish.</p>
<p>Follow the development on <a href="https://github.com/tstullich/renderer">Github</a> or through my <a href="/posts/">blog posts</a>.</p>
<h3 id="2-odin---a-vulkan-based-path-tracer">2. Odin - A Vulkan-based Path Tracer<a href="#2-odin---a-vulkan-based-path-tracer" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p><code>C++</code> <code>GLSL</code> <code>Vulkan</code> <code>Realtime Rendering</code> <code>Shaders</code></p>
<p>This project was my first step into getting my feet wet with the Vulkan API. When NVIDIA
announced their first GPU series featuring the
<a href="https://www.nvidia.com/en-us/geforce/turing/">Turing architecture</a>, I decided to test if it was
possible to write a real-time raytracing application without a dedicated GPU.</p>
<p>The renderer uses a single compute shader to perform the path tracing algorithm and write the result
into a texture image which can be sampled from the graphics pipeline. It also sports the following
features:</p>
<ul>
<li>Support for diffuse, caustic, and dielectric materials</li>
<li>Tracing of geometry serialized in the OBJ file format</li>
<li>A basic Bounding Volume Hierarchy using axis-aligned bounding boxes</li>
</ul>
<p>Overall, the implementation is still quite bottlenecked due to the fact that there are a lot of branching
instructions, which only lets the renderer run at around 10-20 FPS on my hardware. If I could spend some
time optimizing my shader code, I could envision the renderer having more practical applications.</p>
<p>The source code can be found <a href="https://github.com/tstullich/odin">here</a>.</p>
<h3 id="3-raytracing-in-one-weekend-in-rust">3. Raytracing In One Weekend In Rust<a href="#3-raytracing-in-one-weekend-in-rust" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p><code>Rust</code> <code>Raytracing</code></p>

  <figure class="center" >
    <img src="/img/rust-pt.png"  alt="Rust Path Tracer"   style="border-radius: 8px;"  />
    
      <figcaption class="center" >An image created with the Rust path tracer at 1200x800 resolution and 1024spp</figcaption>
    
  </figure>


<p>As a challenge to myself to learn Rust, I decided to adapt Peter Shirley&rsquo;s
<a href="https://raytracing.github.io/">Raytracing In One Weekend</a> tutorial, which is written in C++, into a
Rust application. All of the features from the first book are available including some extras:</p>
<ul>
<li>OBJ model parsing or randomized scene creation</li>
<li>Support for configurable dielectric, diffuse, and caustic materials</li>
<li>Motion blur</li>
<li>Checkered textures support</li>
<li>Configuring of rendering parameters through command line arguments</li>
<li>Multithreaded rendering through the use of Rust&rsquo;s <code>rayon</code> library</li>
</ul>
<p>The source code can be found <a href="https://github.com/tstullich/rust-pt">here</a>.</p>
<h2 id="academic-projects">Academic Projects<a href="#academic-projects" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<h3 id="1-masters-thesis-on-differentiable-rendering">1. Master&rsquo;s Thesis On Differentiable Rendering<a href="#1-masters-thesis-on-differentiable-rendering" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p><code>C++</code> <code>Python</code> <code>Differentiable Rendering</code> <code>Machine Learning</code></p>

  <figure class="center" >
    <img src="/img/redner1.png"  alt="Redner"   style="border-radius: 8px;"  />
    
      <figcaption class="center" >Differentiable rendering tries to minimize the loss between two images by applying gradient descent</figcaption>
    
  </figure>


<p>For my Master&rsquo;s thesis I decided to explore differentiable rendering, which combines machine learning
with computer graphics. For a good explanation on the topic I encourage you to watch
Wenzel Jakob&rsquo;s <a href="https://www.youtube.com/watch?v=prZJ8FBG9BI&amp;t=1895s">Keynote Speech</a>
during the High Performance Graphics 2020 conference.</p>
<p>In my case, I chose to extend the capabilities of the differentiable renderer <code>redner</code>, which was
introduced by Li et al. in the paper <a href="https://people.csail.mit.edu/tzumao/diffrt/">Differentiable
Monte Carlo Ray Tracing through Edge Sampling</a>,
to be able to optimize homogeneous participating media.</p>
<p>The figure above shows one of the test setups that was used to verify the code that I had written.
The perturbed scene on the left-hand side is enveloped in a thick fog and the goal of the test run
was to remove the fog entirely by optimizing the scene towards the target image on the right-hand side.</p>
<p>The result of running the gradient descent optimization can be seen in the figure below. The fog has been
nearly removed from the perturbed image, with some visual artifacts remaining in the tree line in the
background.</p>

  <figure class="center" >
    <img src="/img/redner2.png"  alt="Redner"   style="border-radius: 8px;"  />
    
      <figcaption class="center" >A capture of the results from optimizing homogeneous participating media through redner</figcaption>
    
  </figure>


<p>For more details on my thesis please read through my <a href="/pdf/masters-presentation.pdf">thesis presentation</a> or contact me directly for a copy of the full thesis. The source code can be found <a href="https://github.com/tstullich/redner">here</a>.</p>
<h3 id="3-computer-graphics-2-tu-berlin">3. Computer Graphics 2 (TU Berlin)<a href="#3-computer-graphics-2-tu-berlin" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p><code>OpenGL</code> <code>Qt5</code> <code>Data Structures</code> <code>Surface Reconstruction</code> <code>Implicit Surfaces</code></p>
<p>The Computer Graphics 2 course taught a number of different techniques focused around geometric reconstruction
and implicit surfaces. All of the assignments focused on a particular topic within those domains and were completed in collaboration with two group members. For the coding portions of the assignments we
used OpenGL to perform our graphics computations, with Qt5 acting as the framework for the demos&rsquo; user
interface. Qt5 allowed us to dynamically adjust the parameters for the various demonstrations.
What follows are some of the highlights of the course.</p>
<h4 id="k-d-trees-with-nearest-neighbor-point-search">K-d Trees With Nearest Neighbor Point Search<a href="#k-d-trees-with-nearest-neighbor-point-search" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>

  <figure class="center" >
    <img src="/img/kd-tree.png"  alt="Point Cloud Kd Tree"   style="border-radius: 8px;"  />
    
      <figcaption class="center" >The k-d tree data structure being used to perform k-nearest neighbor search on a point cloud</figcaption>
    
  </figure>


<p>This assignment revolved around building a k-d tree data structure, which acts as a binary tree data structure
for k-dimensional point sets. This is accomplished by partitioning the k-dimensional (k=3 in our
case) spatial domain into a half-space. To find a dividing plane for the half-space, the points in
our data sets were sorted using a quicksort algorithm, which I implemented from scratch for efficient sorting.</p>
<p>The k-d tree was then used in our application to implement a N-Nearest Neighbor Search algorithm to collect
an arbitrary set of points near a point in space. The image above shows how a user can select a point in the
data set, and the nearest neighbor search then gathers of a preset amount of points in its neighborhood.
The selected point is highlighted in pink and the n-closest points (N=1617) are shown in green.</p>
<h4 id="surface-reconstruction-using-marching-cubes">Surface Reconstruction Using Marching Cubes<a href="#surface-reconstruction-using-marching-cubes" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>

  <figure class="center" >
    <img src="/img/mc-points.png"  alt="Implicit Point Cloud"   style="border-radius: 8px;"  />
    
      <figcaption class="center" >The model in this picture is represented by a point cloud implicit surface implementation</figcaption>
    
  </figure>


<p>This assignment focused on surface reconstruction using the marching cubes algorithm. From the implicit
surface representation, shown in the above image, a 3D cube is <em>marched</em> along an adjustable fixed
grid. At each step, we test what parts of the cube&rsquo;s surface is inside or outside of the volume by
intersecting a set of 256 precomputed triangle configurations with the surface. If an intersection is
found, a triangle is constructed through the three intersection points of the cube with the surface
and added to a mesh data structure.</p>

  <figure class="center" >
    <img src="/img/mc-dog.png"  alt="Dog Marching Cubes"   style="border-radius: 8px;"  />
    
      <figcaption class="center" >The reconstructed surface model through the use of a Marching Cubes algorithm</figcaption>
    
  </figure>


<p>One such mesh, which was created after one run of the marching cubes algorithm, can be seen above. Although
the mesh is not a perfect representation of the surface, it is possible to create finer approximations
by adjusting the grid subdivision and tuning the nearest neighbor search parameters.</p>
<p>The source code for these projects can be found <a href="http://github.com/tstullich/cg2">here</a>.</p>
<h3 id="4-computer-graphics-1-tu-berlin">4. Computer Graphics 1 (TU Berlin)<a href="#4-computer-graphics-1-tu-berlin" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p><code>JavaScript</code> <code>WebGL</code> <code>Shaders</code> <code>Scene Graphs</code></p>

  <figure class="center" >
    <img src="/img/cg1-raytracer.png"  alt="CG1 Ray Tracer"   style="border-radius: 8px;"  />
    
      <figcaption class="center" >A WebGL ray tracer that I wrote as part of my studies in CG1</figcaption>
    
  </figure>


<p>The very first course that I took for computer graphics was the CG 1 course taught at the
Technical University Berlin. This was an introductory course which covered a classical forward
rendering pipeline from front to back. Among the topics covered were:</p>
<ul>
<li>Scene graph representations and linear transformations</li>
<li>Camera projections</li>
<li>Texture mapping</li>
<li>Various shading techniques (Gourad, Phong, etc.)</li>
<li>Color theory</li>
</ul>
<p>The final assignment for the course is what convinced me to focus on rendering. The
assignment let us write a ray tracer in WebGL which produced the output image seen above.
The simplicity of the ray tracing algorithm, along with the stunning output it provides
drove me to learn more about the subject and soon enough I was deep into the rabbit hole on rendering.</p>
<p>The source code for the ray tracer as well as the other projects can be found
<a href="http://github.com/tstullich/cg1">here</a>.</p>
<h2 id="conference-papers">Conference Papers<a href="#conference-papers" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<h3 id="edbt-2019-demonstration-paper-publication">EDBT 2019 Demonstration Paper Publication<a href="#edbt-2019-demonstration-paper-publication" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p><code>C++</code> <code>Scientific Writing</code> <code>IoT</code> <code>Sensor Networks</code></p>
<p>During my time as a Student Research Assistant with the Database Systems and Information Management Group
at the Technical University in Berlin, I got the chance to work on a demonstration paper that was presented
at the <em>Extending Database Technology</em> conference in 2019. The work was focused on recording and
replaying sensor data on IoT devices. The abstract for the paper has been cited below:</p>
<blockquote>
<p>As the scientific interest in the Internet of Things (IoT) continues to grow, emulating IoT infrastructure
involving a large number of heterogeneous sensors plays a crucial role. Existing research on emulating
sensors is often tailored to specific hardware and/or software, which makes it difficult to reproduce and
extend. In this paper we show how to emulate different kinds of sensors in a unified way that makes
the downstream application agnostic as to whether the sensor data is acquired from real sensors is read
from memory using emulated sensors. We propose the Resense framework that allows for replaying sensor
data using emulated sensors and provides an easy-to-use software for setting up and executing IoT
experiments involving a large number of heterogeneous sensors. We demonstrate various aspects of
Resense in the context of a sports analytics application using real-world sensor data and a set
of Raspberry Pis.</p>
<p>&ndash; <em><!-- raw HTML omitted -->Resense: Transparent Record and Replay of Sensor Data in the Internet of Things<!-- raw HTML omitted --></em></p>
</blockquote>
<p>The full paper can be read <a href="https://openproceedings.org/2019/conf/edbt/EDBT19_paper_319.pdf">here</a>.</p>

      </div></div>

  


  
</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright copyright--user">
        <span>Tim Stullich
        <a href="https://github.com/tstullich"><i class="fa fa-github"></i></a>
        <a href="https://twitter.com/tstullich"><i class="fa fa-twitter"></i></a>
        <a href="https://www.linkedin.com/in/timstullich/"><i class="fa fa-linkedin"></i></a>
        </span>
    
        <span>:: Theme made by <a href="https://twitter.com/panr">panr</a></span>
      </div>
  </div>
</footer>






<script src="/js/menu.js"></script>
<script src="/js/prism.js"></script>





  
</div>

</body>
</html>
